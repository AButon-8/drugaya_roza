[
  {
    "objectID": "index.html#красная-роза",
    "href": "index.html#красная-роза",
    "title": "Другая Роза",
    "section": "«Красная Роза»",
    "text": "«Красная Роза»\n В своей работе я хотела проанализировать перевод пятитомного сборника писем Розы Люксембург, выполненный 2014-2015 годах Маргаритой Школьниксон-Смишко и опубликованный на сайте proza.ru\n\n\n\n\nРоза Люксембург. Источник: Wikipedia.org | (Фотография Карла Пинкау)\n\n\n\nРоза Люксембург (имя при рождении Rosalia Luxenburg), 5 марта 1871 — 15 января 1919.  Имя Розы Люксембург, великой революционерки-интернационалистки, широко известно. В нашей стране оно увековечено в названиях улиц, площадей, клубов, фабрик, учреждений во многих городах. Имя знают, но кем она была и чем занималась, понимает далеко не каждый.  Жизнь Розы Люксембург связана с социал-демократическим движением Польши, России и Германии. Она последовательно отстаивала принципы интернационализма в рабочем движении, выступала против милитаризма и войны. Несколько лет провела в тюрьмах. Принимала активное участие в Ноябрьской революции 1918 г. в Германии.  Вместе с Карлом Либкнехтом была убита в январе 1919 г."
  },
  {
    "objectID": "index.html#luxemburg-r.-gesammelte-briefe",
    "href": "index.html#luxemburg-r.-gesammelte-briefe",
    "title": "Другая Роза",
    "section": "Luxemburg R. Gesammelte Briefe",
    "text": "Luxemburg R. Gesammelte Briefe\n Письма Розы Люксембург публиковались с 1982 по 1993 годы в немецком издательстве DIETZBERLINLuxemburg (1984).\n\n\n\nОбложка одного из томов издания\n\n\n В томах с 1 по 6 собрано более 2700 писем, открыток и телеграмм более чем 150 корреспондентам в Европе и Америке.\nВ «Сборнике писем» представлены все сохранившиеся письма Розы Люксембург деятелям, которые были активными участниками немецкого рабочего движения или были связаны с ним, и которые доступны редакции."
  },
  {
    "objectID": "index.html#содержание-шеститомного-сборника",
    "href": "index.html#содержание-шеститомного-сборника",
    "title": "Другая Роза",
    "section": "Содержание шеститомного сборника",
    "text": "Содержание шеститомного сборника\n\nТом 1 содержит письма Лео Йогихесу, Августу Бебелю, Карлу Каутскому, Минне Каутской, Францу и Еве Меринг, Паулю Лёбе, Роберту и Матильде Зейдель, Кларе Цеткин, Эмилю Эйххорну и Адольфу Гофману за период с 1893 по 1902 год.\nТом 2 содержит письма Карлу Каутскому, Луизе Каутской, Лео Йогихесу, Вильгельму Дитману, Францу Мерингу, Юлиусу Брюнсу, Роберту Зайделю, Артуру Штадтхагену, Курту Эйснеру, Кларе Цеткин, Косте Цеткину, Фридриху Вестмейеру за период с 1903 по 1908 год.\nТом 3 содержит письма Кларе Цеткин, Косте Цеткину, Конраду Хэнишу, Луизе Каутской, Гансу Каутскому, Лео Йогихесу, Паулю Лёбе, Марии Гек, Бранделю Гек, Паулю Леншу, Роберту и Матильде Зейдель за период с 1909 по 1910 год.\nТом 4 содержит письма Францу Мерингу, Вильгельму Диттману, Бранделю Геку, Конраду Хэнишу, Лео Йогихесу, Луизе Каутской, Вальтеру Стоккеру, Фридриху Вестмейеру, Кларе Цеткин, Косте Цеткину, Гертруде Злотко, Юлиану Мархлевскому за период с 1911 по июль 1914 года.\nТом 5 содержит письма Карлу Либкнехту, Софи Либкнехт, Гансу Дифенбаху, Матильде Якоб, Луизе Каутской, Францу Мерингу, Марте Розенбаум, Матильде Вурм, Кларе Цеткин, Косте Цеткин, Эрнсту Мейеру, Гертруде Злоттко, Фанни Езерской за период с августа 1914 года по январь. 1919.\nТом 6 содержит письма Генриетте Роланд Хост, Карлу Радеку, Борису Кричевскому, Йозефу Блоху, Цезарине Войнаровской, Александру Потресову, Луису Будену, Павлу Аксельроду, Адольфу Варскому, Александру Богданову, Паулю Леви, Марии и Адольфу Гек, Косте Цеткину с 1891 по 1918 год.\n\n Перевода сборника писем на русский язык не существует, оригинальный немецкий текст в свободный доступ не выложен. В фондах библиотеки Центра социально-политической истории ГПИБ России (ЦСПИ) имеются 5 первых томов издания. Стоимость шеститомного сборника на сайте издательства https://dietzberlin.de/ составляет на сегодняшний день 299,40 евро.\nВ 2014-2015 годах Маргаритой Школьниксон-Смишко был сделан любительский перевод части писем Розы Люксембург по имеющемуся у нее пятитомному изданию сборника (шестого тома и у нее не было). 184 небольшие повести или миниатюры, как классифицировал их автор, были опубликованы на сайте proza.ru. Это не перевод в классическом понимании этого слова, в тексте нет точных указаний на номера томов и страниц. Вот как описывает автор свой замысел:\n\nСначала я хотела перевести только некоторые особенно понравившиеся письма, но со временем их становилось всё больше, я погрузилась в ту эпоху, и у меня возникла мысль написать о Розе документальную повесть в письмах. О ней уже есть биографическая книга польского автора, но в ней был сделан совсем другой акцент. Хотя о Розе невозможно рассказывать без учёта её участия в политике, стремления изменить общество к лучшему, я постараюсь свести такие сведения к минимуму. Для меня Роза в первую очередь неординарная женщина, о ней и будет повесть.\n\nПри подготовке исследования были использованы и другие труды, посвященные Розе Люксембург, ее современникам и соратникам. Роза Люксембург: Актуальные аспекты политической и научной деятельности (К 85-летию со дня гибели): Международная конференция в Москве 12 февраля 2004 г. (2004) Миллер и Поттхофф (1999)"
  },
  {
    "objectID": "data.html#сбор-данных",
    "href": "data.html#сбор-данных",
    "title": "Данные",
    "section": "Сбор данных",
    "text": "Сбор данных\n 1. Загружаем все необходимые для нашего исследования библиотеки.\n\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(tidytext)\nlibrary(tokenizers)\nlibrary(udpipe)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\n\n\n2. Список переводов писем Розы Люксембург размещен на 4 веб-страницах, по 50 url на каждой. Соберем все 184 ссылки.\n\n# первая страница с ссылками (0-50)\nurl1 &lt;- \"https://proza.ru/avtor/cnamibog&s=0&book=45#45\" \nhtml1 &lt;-  read_html(url1, encoding = \"Windows-1251\") # читаем html, меняем кодировку\n\n#.poemlink - список ссылок на нужные страницы, собранный с помощью SelectorGadget\n\nlist_links1 &lt;- html1 |&gt; \n  html_elements(\".poemlink\") # загружаем список ссылок\n\n\n# вторая страница с ссылками (51-100)\nurl2 &lt;- \"https://proza.ru/avtor/cnamibog&s=50&book=45#45\" \nhtml2 &lt;-  read_html(url2, encoding = \"Windows-1251\") # читаем html, меняем кодировку\n\n#.poemlink - список ссылок на нужные страницы, собранный с помощью SelectorGadget\n\nlist_links2 &lt;- html2 |&gt; \n  html_elements(\".poemlink\") # загружаем список ссылок\n\n\n# третья страница с ссылками (101-150)\nurl3 &lt;- \"https://proza.ru/avtor/cnamibog&s=100&book=45#45\" \nhtml3 &lt;-  read_html(url3, encoding = \"Windows-1251\") # читаем html, меняем кодировку\n\n#.poemlink - список ссылок на нужные страницы, собранный с помощью SelectorGadget\n\nlist_links3 &lt;- html3 |&gt; \n  html_elements(\".poemlink\") # загружаем список ссылок\n\n\n# четвертая страница с ссылками (151-184)\nurl4 &lt;- \"https://proza.ru/avtor/cnamibog&s=150&book=45#45\" \nhtml4 &lt;-  read_html(url4, encoding = \"Windows-1251\") # читаем html, меняем кодировку\n\n#.poemlink - список ссылок на нужные страницы, собранный с помощью SelectorGadget\n\nlist_links4 &lt;- html4 |&gt; \n  html_elements(\".poemlink\") # загружаем список ссылок\n\n\n3. Создадим таблицу с URL-ссылками на все 184 текста.\n\n# Объединяем списки в один, частичный URL\nall_links &lt;- list(list_links1, list_links2, list_links3, list_links4) |&gt; \n  map_dfr(~ tibble(\n    title = .x |&gt; html_text2(),\n    id_text = .x |&gt; html_attr(\"href\")\n  ))\n\n# Выводим объединённую таблицу\nprint(all_links)\n\n# A tibble: 184 × 2\n   title                     id_text         \n   &lt;chr&gt;                     &lt;chr&gt;           \n 1 Другая Роза               /2014/04/05/1003\n 2 второе письмо             /2015/04/18/686 \n 3 Только Дело               /2014/05/01/568 \n 4 Ежовые рукавицы           /2014/04/29/1795\n 5 Как сердцу высказать себя /2014/05/02/722 \n 6 синяки на душе            /2014/05/04/741 \n 7 вторым классом            /2015/04/22/694 \n 8 удачная поездка           /2015/04/23/918 \n 9 середина лета 1898 года   /2015/04/25/1336\n10 неловкие попытки          /2015/04/27/493 \n# ℹ 174 more rows\n\n# Добавляем (paste0 += без пробела) протокол доступа и доменное имя. \n# Получаем полные URL-адреса страниц с главами повести.\nall_links &lt;- all_links |&gt;\n  mutate(link = paste0(\"https://proza.ru\", id_text)) \n# select(-id_text) не делаю, id записи = дате публикации\n\n# Извлекаем список всех ссылок\nall_roza_urls &lt;- all_links |&gt; \n  pull(link)\n\nprint(all_roza_urls)\n\n  [1] \"https://proza.ru/2014/04/05/1003\" \"https://proza.ru/2015/04/18/686\" \n  [3] \"https://proza.ru/2014/05/01/568\"  \"https://proza.ru/2014/04/29/1795\"\n  [5] \"https://proza.ru/2014/05/02/722\"  \"https://proza.ru/2014/05/04/741\" \n  [7] \"https://proza.ru/2015/04/22/694\"  \"https://proza.ru/2015/04/23/918\" \n  [9] \"https://proza.ru/2015/04/25/1336\" \"https://proza.ru/2015/04/27/493\" \n [11] \"https://proza.ru/2015/04/29/543\"  \"https://proza.ru/2014/05/05/1493\"\n [13] \"https://proza.ru/2015/05/01/498\"  \"https://proza.ru/2015/05/02/590\" \n [15] \"https://proza.ru/2015/05/03/561\"  \"https://proza.ru/2014/05/03/843\" \n [17] \"https://proza.ru/2015/05/03/1164\" \"https://proza.ru/2015/05/04/1826\"\n [19] \"https://proza.ru/2015/05/06/548\"  \"https://proza.ru/2015/05/08/651\" \n [21] \"https://proza.ru/2015/05/08/635\"  \"https://proza.ru/2015/05/08/628\" \n [23] \"https://proza.ru/2015/05/10/728\"  \"https://proza.ru/2015/05/10/1312\"\n [25] \"https://proza.ru/2014/05/06/723\"  \"https://proza.ru/2015/05/12/1379\"\n [27] \"https://proza.ru/2014/05/07/640\"  \"https://proza.ru/2015/05/13/446\" \n [29] \"https://proza.ru/2015/05/14/424\"  \"https://proza.ru/2015/05/14/486\" \n [31] \"https://proza.ru/2015/05/15/477\"  \"https://proza.ru/2014/07/01/617\" \n [33] \"https://proza.ru/2015/05/20/643\"  \"https://proza.ru/2015/05/21/764\" \n [35] \"https://proza.ru/2015/05/22/590\"  \"https://proza.ru/2015/05/23/1475\"\n [37] \"https://proza.ru/2014/05/08/1008\" \"https://proza.ru/2014/05/09/777\" \n [39] \"https://proza.ru/2015/05/25/653\"  \"https://proza.ru/2015/05/26/435\" \n [41] \"https://proza.ru/2015/05/26/1411\" \"https://proza.ru/2015/05/27/1002\"\n [43] \"https://proza.ru/2015/05/28/997\"  \"https://proza.ru/2014/05/10/847\" \n [45] \"https://proza.ru/2015/05/30/628\"  \"https://proza.ru/2014/07/17/1227\"\n [47] \"https://proza.ru/2015/05/31/582\"  \"https://proza.ru/2014/07/18/1107\"\n [49] \"https://proza.ru/2015/05/31/854\"  \"https://proza.ru/2014/07/20/590\" \n [51] \"https://proza.ru/2014/07/21/567\"  \"https://proza.ru/2014/04/01/633\" \n [53] \"https://proza.ru/2014/07/23/483\"  \"https://proza.ru/2014/07/24/402\" \n [55] \"https://proza.ru/2014/07/25/477\"  \"https://proza.ru/2014/04/02/1756\"\n [57] \"https://proza.ru/2014/07/27/1162\" \"https://proza.ru/2014/07/28/493\" \n [59] \"https://proza.ru/2014/05/15/905\"  \"https://proza.ru/2014/07/29/664\" \n [61] \"https://proza.ru/2014/05/16/754\"  \"https://proza.ru/2014/07/30/847\" \n [63] \"https://proza.ru/2014/07/31/731\"  \"https://proza.ru/2014/08/03/471\" \n [65] \"https://proza.ru/2014/05/21/1530\" \"https://proza.ru/2014/08/01/525\" \n [67] \"https://proza.ru/2014/08/02/789\"  \"https://proza.ru/2014/08/04/631\" \n [69] \"https://proza.ru/2014/05/26/681\"  \"https://proza.ru/2014/05/24/1684\"\n [71] \"https://proza.ru/2014/05/25/679\"  \"https://proza.ru/2014/05/28/953\" \n [73] \"https://proza.ru/2014/05/29/614\"  \"https://proza.ru/2014/05/30/761\" \n [75] \"https://proza.ru/2014/05/31/518\"  \"https://proza.ru/2014/06/01/500\" \n [77] \"https://proza.ru/2014/06/01/515\"  \"https://proza.ru/2014/06/02/557\" \n [79] \"https://proza.ru/2014/06/03/566\"  \"https://proza.ru/2015/06/05/1508\"\n [81] \"https://proza.ru/2014/06/04/1629\" \"https://proza.ru/2015/06/06/702\" \n [83] \"https://proza.ru/2014/06/07/453\"  \"https://proza.ru/2014/06/08/409\" \n [85] \"https://proza.ru/2014/06/09/352\"  \"https://proza.ru/2015/06/07/502\" \n [87] \"https://proza.ru/2014/06/10/784\"  \"https://proza.ru/2014/06/13/944\" \n [89] \"https://proza.ru/2014/06/14/937\"  \"https://proza.ru/2014/06/16/1809\"\n [91] \"https://proza.ru/2014/06/19/1680\" \"https://proza.ru/2014/06/18/827\" \n [93] \"https://proza.ru/2014/06/20/608\"  \"https://proza.ru/2014/06/21/1239\"\n [95] \"https://proza.ru/2014/06/22/499\"  \"https://proza.ru/2014/06/23/822\" \n [97] \"https://proza.ru/2014/06/24/591\"  \"https://proza.ru/2014/06/26/1056\"\n [99] \"https://proza.ru/2014/06/27/499\"  \"https://proza.ru/2014/06/28/703\" \n[101] \"https://proza.ru/2014/06/29/581\"  \"https://proza.ru/2014/06/30/701\" \n[103] \"https://proza.ru/2014/07/02/761\"  \"https://proza.ru/2014/04/22/1370\"\n[105] \"https://proza.ru/2014/07/03/917\"  \"https://proza.ru/2014/07/04/753\" \n[107] \"https://proza.ru/2014/07/06/610\"  \"https://proza.ru/2014/07/07/676\" \n[109] \"https://proza.ru/2014/07/08/732\"  \"https://proza.ru/2014/07/09/494\" \n[111] \"https://proza.ru/2014/07/10/812\"  \"https://proza.ru/2014/07/11/674\" \n[113] \"https://proza.ru/2014/07/12/491\"  \"https://proza.ru/2014/07/13/667\" \n[115] \"https://proza.ru/2014/07/15/727\"  \"https://proza.ru/2014/08/05/578\" \n[117] \"https://proza.ru/2014/08/07/962\"  \"https://proza.ru/2014/08/13/839\" \n[119] \"https://proza.ru/2014/08/08/805\"  \"https://proza.ru/2014/08/06/794\" \n[121] \"https://proza.ru/2014/08/09/954\"  \"https://proza.ru/2014/08/10/485\" \n[123] \"https://proza.ru/2014/08/11/455\"  \"https://proza.ru/2014/08/11/820\" \n[125] \"https://proza.ru/2014/08/14/607\"  \"https://proza.ru/2014/08/15/745\" \n[127] \"https://proza.ru/2014/08/16/753\"  \"https://proza.ru/2014/08/16/1720\"\n[129] \"https://proza.ru/2014/08/16/1934\" \"https://proza.ru/2014/08/18/1273\"\n[131] \"https://proza.ru/2014/08/23/645\"  \"https://proza.ru/2014/08/24/961\" \n[133] \"https://proza.ru/2014/08/25/602\"  \"https://proza.ru/2014/08/29/1642\"\n[135] \"https://proza.ru/2014/09/03/816\"  \"https://proza.ru/2014/09/06/779\" \n[137] \"https://proza.ru/2014/09/07/671\"  \"https://proza.ru/2014/09/08/1276\"\n[139] \"https://proza.ru/2014/09/09/631\"  \"https://proza.ru/2014/09/11/468\" \n[141] \"https://proza.ru/2014/09/12/498\"  \"https://proza.ru/2014/09/13/530\" \n[143] \"https://proza.ru/2014/09/13/521\"  \"https://proza.ru/2014/09/14/1444\"\n[145] \"https://proza.ru/2014/09/19/776\"  \"https://proza.ru/2014/09/20/495\" \n[147] \"https://proza.ru/2014/09/22/604\"  \"https://proza.ru/2014/09/23/907\" \n[149] \"https://proza.ru/2014/09/24/426\"  \"https://proza.ru/2014/09/26/639\" \n[151] \"https://proza.ru/2014/09/27/486\"  \"https://proza.ru/2014/09/28/873\" \n[153] \"https://proza.ru/2014/04/06/794\"  \"https://proza.ru/2014/04/13/1979\"\n[155] \"https://proza.ru/2014/10/05/637\"  \"https://proza.ru/2014/04/09/1602\"\n[157] \"https://proza.ru/2014/10/08/885\"  \"https://proza.ru/2014/10/12/682\" \n[159] \"https://proza.ru/2014/10/16/804\"  \"https://proza.ru/2014/10/19/2165\"\n[161] \"https://proza.ru/2014/10/22/1036\" \"https://proza.ru/2014/10/23/592\" \n[163] \"https://proza.ru/2014/10/25/728\"  \"https://proza.ru/2014/10/23/1680\"\n[165] \"https://proza.ru/2014/04/24/1767\" \"https://proza.ru/2014/04/23/943\" \n[167] \"https://proza.ru/2014/10/26/933\"  \"https://proza.ru/2014/10/27/1004\"\n[169] \"https://proza.ru/2014/10/28/870\"  \"https://proza.ru/2014/10/29/714\" \n[171] \"https://proza.ru/2014/11/05/710\"  \"https://proza.ru/2014/11/06/705\" \n[173] \"https://proza.ru/2014/11/08/838\"  \"https://proza.ru/2014/11/08/2010\"\n[175] \"https://proza.ru/2014/11/10/918\"  \"https://proza.ru/2014/11/16/1044\"\n[177] \"https://proza.ru/2014/11/20/1052\" \"https://proza.ru/2014/11/22/1117\"\n[179] \"https://proza.ru/2014/11/23/845\"  \"https://proza.ru/2014/11/25/990\" \n[181] \"https://proza.ru/2014/11/27/584\"  \"https://proza.ru/2014/11/27/839\" \n[183] \"https://proza.ru/2014/11/28/947\"  \"https://proza.ru/2014/04/06/985\" \n\n\n\n4. Скрапинг текста\n\n# Напишем функцию для скрапинга текста\nget_text &lt;- function(url){\n  read_html(url, encoding = \"Windows-1251\") |&gt; \n    html_elements(\".text\") |&gt; \n    html_text2() |&gt; \n    paste(collapse=\" \")\n}\n\n# Применим функцию к полному списку извлеченных ссылок. \n# Получаем Large list.\nraw_all_roza_texts &lt;- map(all_roza_urls, get_text)\n\n# Превращаем список в символьный вектор, а его в таблицу\nraw_roza_texts_tbl &lt;- raw_all_roza_texts |&gt; \n  flatten_chr() |&gt; \n  as_tibble()\n\n# Объединяем две таблицы (ссылки, названия текстов и сами тексты)\nall_roza_proza &lt;- all_links |&gt; \n  mutate(text = raw_roza_texts_tbl)"
  },
  {
    "objectID": "data.html#предобработка-данных",
    "href": "data.html#предобработка-данных",
    "title": "Данные",
    "section": "Предобработка данных",
    "text": "Предобработка данных\n Чистим данные с помощью регулярных выражений.\n\n# Переименовываем столбец\n# Удаляем HTML-теги\n# Унифицируем кавычки, апострофы\n# Убираем лишние переводы строки (включая пробелы вокруг них), заменяем их на пробел\n# Убираем звездочки перед словами (от примечаний)\n# Ставим перед записанными с ошибками именами МАРКЕР\nall_roza_cleaned &lt;- all_roza_proza |&gt;\n  mutate(text = text$value) |&gt; \n  mutate(text = str_remove_all(as.character(text), \"&lt;[^&gt;]+&gt;\")) |&gt;\n  mutate(text = str_replace_all(text, \"[«»„”‘’]\", \"\\\"\")) |&gt; \n  mutate(text = gsub(\"\\\\s*\\\\n+\\\\s*\", \" \", text)) |&gt; \n  mutate(text = gsub(\"\\\\b([Нн]и{1,2}у\\\\S*)\", \"NIU_\\\\1\", text)) |&gt; \n  mutate(text = gsub(\"\\\\b([Кк]ост\\\\S*)\", \"KOSTYA_\\\\1\", text)) |&gt; \n  mutate(text = gsub(\"\\\\b([Юю]{2}\\\\S*)\", \"YuYu_\\\\1\", text)) |&gt;\n  mutate(text = gsub(\"^[*]+\", \"\", text)) |&gt; \n  mutate(text = str_replace_all(text, \"\\\\b([Рр]оз(?!енфельд|енталь)(\\\\S*))\", \"ROZA_\\\\1\")) |&gt; \n  mutate(text = gsub(\"\\\\b([Мм]ими)\", \"MIMI_\\\\1\", text)) |&gt; \n  mutate(text = gsub(\"\\\\b([Дд][Цц]ио|[Цц]иу|[Дд]иу)\\\\S*\", \"DZIO_\\\\1\", text)) |&gt; \n  mutate(text = gsub(\"\\\\b([Шш]ое|[Шш]оэ|[Шш]\\\\.)\\\\S*\", \"Schönlank_\\\\1\", text)) |&gt; \n  mutate(text = gsub(\"\\\\b([Мм]атиль\\\\S*)\", \"MATILDA_\\\\1\", text)) |&gt; \n  mutate(text = gsub(\"\\\\b([Лл]улу)\", \"LULU_\\\\1\", text))"
  },
  {
    "objectID": "index.html#коротко-о-переводчике",
    "href": "index.html#коротко-о-переводчике",
    "title": "Другая Роза",
    "section": "Коротко о переводчике",
    "text": "Коротко о переводчике\n О Маргарите Школьниксон-Смишко известно совсем немного. Активно ведет литературный дневник на proza.ru. С начала 2010-х годов на сайте ею было опубликовано более 1590 произведений, которые прочитали 174463 человек (данные на декабрь 2024 года).\n \nЗдесь она публикует воспоминания,\n\nв том числе людей с неординарной судьбой\n\nписьма,\n\nначиная с моих к будущему мужу (“Письма немецкому другу”) и, конечно, более значимых писем друзьям и любимым Розы Люксембург\n\nпереводы.\n\nперевожу то, что мне созвучно, интересно и считаю важным донести до многих."
  },
  {
    "objectID": "data.html#аннотирование-текста",
    "href": "data.html#аннотирование-текста",
    "title": "Данные",
    "section": "Аннотирование текста",
    "text": "Аннотирование текста\n Скачиваем и загружаем модель. Аннотируем.\n\n# Создаем папку models, если её ещё нет\nif (!dir.exists(\"models\")) {\n  dir.create(\"models\")\n}\n\n# Добавляем models/ в .gitignore, если её там нет\nif (!file.exists(\".gitignore\")) {\n  write(\"models/\", file = \".gitignore\")\n} else {\n  # Читаем содержимое .gitignore\n  gitignore_content &lt;- readLines(\".gitignore\")\n  \n  # Проверяем, есть ли уже models/\n  if (!\"models/\" %in% gitignore_content) {\n    # Добавляем строку в конец файла, если её нет\n    write(\"models/\", file = \".gitignore\", append = TRUE)\n  }\n}\n\n# Указываем путь к модели\nmodel_path &lt;- \"models/russian-syntagrus-ud-2.5-191206.udpipe\"\n\n# Проверяем, существует ли файл модели\nif (!file.exists(model_path)) {\n  message(\"Скачиваем модель...\")\n  udpipe_download_model(language = \"russian-syntagrus\", model_dir = \"models\")\n} else {\n  message(\"Модель уже скачана.\")\n}\n\n# Загружаем модель\nrussian_syntagrus &lt;- udpipe_load_model(file = model_path)\n\n# Отобразим текст в документе Quarto\ncat(\"Модель успешно загружена и готова к использованию.\")\n\nМодель успешно загружена и готова к использованию.\n\n\n\n# Скачиваем модель в рабочую директорию\n##| echo: false\n# udpipe_download_model(language = \"russian-syntagrus\")\n\n# загружаем модель\n##| echo: false\n#russian_syntagrus &lt;- udpipe_load_model(file = \"russian-syntagrus-ud-2.5-191206.udpipe\")\n\n# Оператор присваивания создает переменную params.\nparams &lt;- tribble(\n  ~tbl, ~output, ~input, ~token,\n  all_roza_cleaned, \"word\", \"text\", \"words\"\n)\n\nparams\n\n\n  \n\n\n# аннотируем\n##| echo: false\nall_roza_annotate &lt;- udpipe_annotate(\n  russian_syntagrus, \n  all_roza_cleaned$text, \n  doc_id = all_roza_proza$title)\n\n# преобразуем аннотированное в тибл\n##| echo: false\nall_roza_ann_tbl &lt;- as_tibble(all_roza_annotate) |&gt; \n  select(-paragraph_id, -sentence, -xpos) |&gt; \n  as_tibble()"
  },
  {
    "objectID": "data.html#токенизация",
    "href": "data.html#токенизация",
    "title": "Данные",
    "section": "Токенизация",
    "text": "Токенизация\n\n1. Токенизируем.\n\nall_roza_tokens &lt;- all_roza_cleaned |&gt; \n  unnest_tokens(\"word\",\n                \"text\",\n                to_lower = TRUE,\n                strip_punct = TRUE) |&gt; \n  select(-id_text)\n\nall_roza_tokens\n\n\n  \n\n\n\n\n2. Создаем списки стоп-слов\n\n# Загружаем список список стоп-слов ('nltk')\nlibrary(stopwords)\nstopwords(\"ru\")\n\n  [1] \"и\"       \"в\"       \"во\"      \"не\"      \"что\"     \"он\"      \"на\"     \n  [8] \"я\"       \"с\"       \"со\"      \"как\"     \"а\"       \"то\"      \"все\"    \n [15] \"она\"     \"так\"     \"его\"     \"но\"      \"да\"      \"ты\"      \"к\"      \n [22] \"у\"       \"же\"      \"вы\"      \"за\"      \"бы\"      \"по\"      \"только\" \n [29] \"ее\"      \"мне\"     \"было\"    \"вот\"     \"от\"      \"меня\"    \"еще\"    \n [36] \"нет\"     \"о\"       \"из\"      \"ему\"     \"теперь\"  \"когда\"   \"даже\"   \n [43] \"ну\"      \"вдруг\"   \"ли\"      \"если\"    \"уже\"     \"или\"     \"ни\"     \n [50] \"быть\"    \"был\"     \"него\"    \"до\"      \"вас\"     \"нибудь\"  \"опять\"  \n [57] \"уж\"      \"вам\"     \"сказал\"  \"ведь\"    \"там\"     \"потом\"   \"себя\"   \n [64] \"ничего\"  \"ей\"      \"может\"   \"они\"     \"тут\"     \"где\"     \"есть\"   \n [71] \"надо\"    \"ней\"     \"для\"     \"мы\"      \"тебя\"    \"их\"      \"чем\"    \n [78] \"была\"    \"сам\"     \"чтоб\"    \"без\"     \"будто\"   \"человек\" \"чего\"   \n [85] \"раз\"     \"тоже\"    \"себе\"    \"под\"     \"жизнь\"   \"будет\"   \"ж\"      \n [92] \"тогда\"   \"кто\"     \"этот\"    \"говорил\" \"того\"    \"потому\"  \"этого\"  \n [99] \"какой\"   \"совсем\"  \"ним\"     \"здесь\"   \"этом\"    \"один\"    \"почти\"  \n[106] \"мой\"     \"тем\"     \"чтобы\"   \"нее\"     \"кажется\" \"сейчас\"  \"были\"   \n[113] \"куда\"    \"зачем\"   \"сказать\" \"всех\"    \"никогда\" \"сегодня\" \"можно\"  \n[120] \"при\"     \"наконец\" \"два\"     \"об\"      \"другой\"  \"хоть\"    \"после\"  \n[127] \"над\"     \"больше\"  \"тот\"     \"через\"   \"эти\"     \"нас\"     \"про\"    \n[134] \"всего\"   \"них\"     \"какая\"   \"много\"   \"разве\"   \"сказала\" \"три\"    \n[141] \"эту\"     \"моя\"     \"впрочем\" \"хорошо\"  \"свою\"    \"этой\"    \"перед\"  \n[148] \"иногда\"  \"лучше\"   \"чуть\"    \"том\"     \"нельзя\"  \"такой\"   \"им\"     \n[155] \"более\"   \"всегда\"  \"конечно\" \"всю\"     \"между\"  \n\nstopwords_ru &lt;- c(stopwords(\"ru\", source = \"nltk\"))\n\n# Убираем повторы и упорядочиваем по алфавиту\nstopwords_ru &lt;- sort(unique(stopwords_ru))\nstopwords_ru\n\n  [1] \"а\"       \"без\"     \"более\"   \"больше\"  \"будет\"   \"будто\"   \"бы\"     \n  [8] \"был\"     \"была\"    \"были\"    \"было\"    \"быть\"    \"в\"       \"вам\"    \n [15] \"вас\"     \"вдруг\"   \"ведь\"    \"во\"      \"вот\"     \"впрочем\" \"все\"    \n [22] \"всегда\"  \"всего\"   \"всех\"    \"всю\"     \"вы\"      \"где\"     \"да\"     \n [29] \"даже\"    \"два\"     \"для\"     \"до\"      \"другой\"  \"его\"     \"ее\"     \n [36] \"ей\"      \"ему\"     \"если\"    \"есть\"    \"еще\"     \"ж\"       \"же\"     \n [43] \"за\"      \"зачем\"   \"здесь\"   \"и\"       \"из\"      \"или\"     \"им\"     \n [50] \"иногда\"  \"их\"      \"к\"       \"как\"     \"какая\"   \"какой\"   \"когда\"  \n [57] \"конечно\" \"кто\"     \"куда\"    \"ли\"      \"лучше\"   \"между\"   \"меня\"   \n [64] \"мне\"     \"много\"   \"может\"   \"можно\"   \"мой\"     \"моя\"     \"мы\"     \n [71] \"на\"      \"над\"     \"надо\"    \"наконец\" \"нас\"     \"не\"      \"него\"   \n [78] \"нее\"     \"ней\"     \"нельзя\"  \"нет\"     \"ни\"      \"нибудь\"  \"никогда\"\n [85] \"ним\"     \"них\"     \"ничего\"  \"но\"      \"ну\"      \"о\"       \"об\"     \n [92] \"один\"    \"он\"      \"она\"     \"они\"     \"опять\"   \"от\"      \"перед\"  \n [99] \"по\"      \"под\"     \"после\"   \"потом\"   \"потому\"  \"почти\"   \"при\"    \n[106] \"про\"     \"раз\"     \"разве\"   \"с\"       \"сам\"     \"свою\"    \"себе\"   \n[113] \"себя\"    \"сейчас\"  \"со\"      \"совсем\"  \"так\"     \"такой\"   \"там\"    \n[120] \"тебя\"    \"тем\"     \"теперь\"  \"то\"      \"тогда\"   \"того\"    \"тоже\"   \n[127] \"только\"  \"том\"     \"тот\"     \"три\"     \"тут\"     \"ты\"      \"у\"      \n[134] \"уж\"      \"уже\"     \"хорошо\"  \"хоть\"    \"чего\"    \"чем\"     \"через\"  \n[141] \"что\"     \"чтоб\"    \"чтобы\"   \"чуть\"    \"эти\"     \"этого\"   \"этой\"   \n[148] \"этом\"    \"этот\"    \"эту\"     \"я\"      \n\n# Добавляем дополнительные стоп-слова\nother &lt;- c(\" \", \"\", \"это\", \"который\", \"роза\", \n           \"весь\", \"мочь\", \"свой\", \"твой\", \"очень\", \n           \"каждый\", \"ваш\", \"изза\", \"поэтому\", \n           \"хотя\", \"сразу\", \"наш\", \"все\", \"еще\", \"ее\", \"её\", \n           \"тебе\", \"твое\", \"кроме\", \"мои\", \"dieser\", \"in\", \n           \"der\", \"dir\", \"dein\", \"die\", \"den\", \"ich\", \"und\",\n           \"всё\", \"ещё\", \"твоё\", \"моё\", \"неё\", \"которые\", \n           \"моей\", \"лишь\", \"своей\", \"моего\", \"которых\", \n           \"таких\", \"таким\", \"своими\", \"ними\", \"также\", \"мной\", \n           \"крайней\", \"мере\", \"конце\", \"концов\", \"которой\", \n           \"которое\", \"вообще\", \"свои\", \"которая\", \"например\", \n           \"такие\", \"этим\", \"такую\", \"эта\", \"каким\", \"которую\", \n           \"to\")\n\n\n3. Удаляем стоп-слова. Делаем две tidy таблицы\n\n# Делаем tidy таблицу с лемматизированными данными: \n# Переименовываем колонку lemma\nall_roza_ann_tbl &lt;- all_roza_ann_tbl |&gt; \n  mutate(word = lemma)\n\n# Переводим колонку word в нижний регистр\n# Удаляем все знаки препинания из столбца word\n# Убираем все комбинации \".»\", встречающиеся перед границей слова в столбце lemma\n# Удаляем стоп слова + доп. стоп-слова\nall_roza_tidy &lt;- all_roza_ann_tbl |&gt;\n  mutate(word = tolower(str_trim(word))) |&gt;\n  mutate(word = gsub(\"[[:punct:]]\", \"\", word)) |&gt; \n  mutate(lemma = gsub(\"\\\\.»\\\\b\", \"\", lemma)) |&gt;\n  anti_join(tibble(word = stopwords_ru)) |&gt; \n  filter(!word %in% other)\n\nall_roza_tidy\n\n\n  \n\n\n# Делаем tidy таблицу с токенизированными данными\n# Необходимость в токенизированной таблице возникала из-за грязных исходных данных.\n# Невычитанный текст с множеством опечаток и ошибок неверно лемматизировался.\nrp_token_tidy &lt;- all_roza_tokens |&gt; \n  anti_join(tibble(word = stopwords_ru)) |&gt; \n  filter(!word %in% other)\n\nrp_token_tidy"
  },
  {
    "objectID": "personalities.html#имена-собственные",
    "href": "personalities.html#имена-собственные",
    "title": "Персоналии",
    "section": "Имена собственные",
    "text": "Имена собственные\n В начале исследования с помощью аннотированных данных были выбраны все имена собственные.\n\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(tidytext)\nlibrary(tokenizers)\nlibrary(udpipe)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(wordcloud)\nlibrary(readr)\n\n# Загружаем тибл, полученный на прошлом этапе исследования.\nall_roza_ann_tbl &lt;- read.csv(\"docs/all_roza_ann_tbl.csv\")\n\n# Находим все имена собственные в аннотированном тексте, считаем леммы\npropn_list &lt;- all_roza_ann_tbl |&gt;\n  filter(upos == \"PROPN\") |&gt; \n  filter(str_detect(feats, \"Animacy=Inan\")) |&gt; \n  count(lemma) |&gt; \n  arrange(-n)\n\n# Выводим топ-20 самых частых лемм с признаками Animacy=Inan\n# head(propn_list, 20)\n\nРезультаты первых рассчетов можно посмотреть в прошлом домашем задании\nМы получили неопрятный список географических названий и имен. Даже поверхностный анализ данных показал, что лемматизация дает много ошибочных результатов. Исходный текст изобилует опечатками, вариантами написания имен, уменьшительно-ласкательными формами. Вернувшись на этап “чистки” данных, к проблемных именам были добавлены “маркеры” (Например, “DZIO_” или “NIU_”). Дальше вручную был составлен список упоминавшихся имен и собраны данные.\n\n# Посчитаем упоминаемых персон (+МАРКЕРЫ)\npersona2_qty &lt;- all_roza_ann_tbl |&gt; \n  filter(lemma == \"Клара\" | lemma == \"ROZA_Роз\" | lemma == \"NIU_Ниуниу\" \n         | lemma == \"Цеткин\" | lemma == \"MATILDA_Матильд\" | lemma == \"Дифенбах\"\n         | lemma == \"Люксембург\" | lemma == \"DZIO_Дцио\" | lemma == \"Вронка\"\n         | lemma == \"Меринг\" | lemma == \"MATILDA_Матильда\"| lemma == \"Бебель\" \n         | lemma == \"дорогой\" | lemma == \"сон\" | lemma == \"Гертруд\" \n         | lemma == \"Либкнехта\" | lemma == \"Парвус\" | lemma == \"Вурм\"\n         | lemma == \"Розенфельд\" | lemma == \"KOSTYA_Кость\" | lemma == \"ROZA_Розин\"\n         | lemma == \"YuYu_Юя\" | lemma == \"Карл\" | lemma == \"Толст\"\n         | lemma == \"ROZA_Роза\" | lemma == \"Марта\" | lemma == \"Пауль\"\n         | lemma == \"Троиц\" | lemma == \"Фаисст\" | lemma == \"Ханс\"\n         | lemma == \"KOSTYA_Кост\" | lemma == \"Розенталь\" | lemma == \"ROZA_Розино\"\n         | lemma == \"DZIO_Циу\" | lemma == \"LULU_Лул\" | lemma == \"NIU_Ниуниус\" \n         | lemma == \"ROZA_роз\" | lemma == \"Бах\" | lemma == \"Берндштейн\"\n         | lemma == \"Турнер\" | lemma == \"Бы\" | lemma == \"Гольдберг\" \n         | lemma == \"Дорогуль\" | lemma == \"Каролус\" | lemma == \"Мария\" \n         | lemma == \"Мёрика\" | lemma == \"Рикарда\" | lemma == \"Роберт\" \n         | lemma == \"Соня\" | lemma == \"NIU_Ниуниа\" | lemma == \"YuYu_Ююк\" \n         | lemma == \"Августина\" | lemma == \"Боутен\" | lemma == \"Веггис\" \n         | lemma == \"Вильгельм\" | lemma == \"    Вронкэ\" | lemma == \"Голсуорси\" \n         | lemma == \"Грефенсберг\" | lemma == \"Ид\" | lemma == \"Каспрзак\" \n         | lemma == \"Каутский\" | lemma == \"Кольберг\" | lemma == \"   Луиза\" \n         | lemma == \"Максим\" | lemma == \"Моцарт\" | lemma == \"Н\" \n         | lemma == \"Р\" | lemma == \"Струв\" | lemma == \"Тильд\" \n         | lemma == \"Феликс\" | lemma == \"Форст\" | lemma == \"Цитц\" \n         | lemma == \"Шёнеберг\" | lemma == \"Эбенхаузен\" | lemma == \"**Парвус\" \n         | lemma == \"*ROZA_Розино\" | lemma == \"KOSTYA_Костик\" \n         | lemma == \"LULU_лул\" | lemma == \"NIU_Ниунисия\" | lemma == \"NIU_Ниуниуш\" \n         | lemma == \"ROZA_Розе\" | lemma == \"ROZA_Розина\" | lemma == \"ROZA_Розины\" \n         | lemma == \"YuYu_Ююзия\" | lemma == \"Анна\" | lemma == \"Бернштейн\" \n         | lemma == \"К.\" | lemma == \"Лигниц\" | lemma == \"Ми\" | lemma == \"Нинуниу\" \n         | token == \"Мими\" | lemma == \"Юю\") |&gt; \n  select(doc_id, lemma, token, upos, feats) |&gt;\n  count(lemma, sort = TRUE)\n\nhead(propn_list, 20)\n\n\n  \n\n\n\nПодробнее о нескольких из упоминаемых персоналий."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Исследования",
    "section": "",
    "text": "## Векторное представление слов\nСтроим эмбеддинги, взяв за основу матрицу термин-термин.\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(tidytext)\nlibrary(tokenizers)\nlibrary(udpipe)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(wordcloud)\nlibrary(readr)\n1. Скользящее окно\n# Загружаем заранее сохраненные данные\nall_roza_tidy &lt;- read.csv(\"docs/all_roza_tidy.csv\")\nrp_token_tidy &lt;- read.csv(\"docs/rp_token_tidy.csv\")\n\n# \"Гнездуем\")) токены по главам повести для дальнейшего деления на окна\n#nested_proza &lt;- all_roza_tokens |&gt; \n#  dplyr::select(-link) |&gt; \n#  nest(token = c(word))\n\n# NEW \"Гнездуем\" почищенные токены\nclean_nested_proza &lt;- rp_token_tidy |&gt; \n  dplyr::select(-link) |&gt; \n  nest(token = c(word))\n2. Создаем функцию для деления со сдвигом\nslide_windows &lt;- function(tbl, window_size) {\n  skipgrams &lt;- slider::slide(\n    tbl, \n    ~.x, \n    .after = window_size - 1, \n    .step = 1, \n    .complete = TRUE\n  )\n  \n  safe_mutate &lt;- safely(mutate)\n  \n  out &lt;- map2(.x = skipgrams,\n              .y = 1:length(skipgrams), # Генерация последовательности индексов\n              ~ safe_mutate(.x, window_id = .y))\n  \n  out |&gt; \n    transpose() |&gt; \n    pluck(\"result\") |&gt; \n    compact() |&gt; \n    bind_rows()\n}\n3. Делим на окна\n# Ширина окна 10L - .after = window_size - 1 дает 11.\n# unnest - распаковываем токены\n# unite - достаем слова из столбца со словами и у нас отдельно window_id и название документа\n#proza_windows &lt;- nested_proza |&gt; \n#  mutate(token = map(token, slide_windows, 10L)) |&gt;  \n#  unnest(token) |&gt;  \n#  unite(window_id, title, window_id)\n\n\n# NEW То же самое делаем с почищенным файлом\nclean_proza_windows &lt;- clean_nested_proza |&gt; \n  mutate(token = map(token, slide_windows, 10L)) |&gt;  \n  unnest(token) |&gt;  \n  unite(window_id, title, window_id)\n4. Считаем PMI и PPMI\nlibrary(widyr)\n\nclean_rp_pmi &lt;- clean_proza_windows |&gt;\n  pairwise_pmi(word, window_id)\n\n# всего значений/токенов/координат\nclean_rp_pmi$item1 |&gt; unique() |&gt; length() # 20710\n\n[1] 20710\n\n# Positive PPMI\nclean_rp_ppmi &lt;- clean_rp_pmi |&gt; \n  mutate(ppmi = case_when(pmi &lt; 0 ~ 0,\n                          .default = pmi))\n\nclean_rp_ppmi |&gt; \n  arrange(pmi)\n5. SVD на матрице с PPMI\nclean_rp_emb &lt;- clean_rp_ppmi |&gt; \n  widely_svd(item1, item2, ppmi,\n             weight_d = FALSE, nv = 100) |&gt; \n  rename(word = item1) # иначе nearest_neighbors() будет жаловаться\n6. Визуализация топиков\nТопики с 1 по 9\nclean_rp_emb |&gt; \n  filter(dimension &lt; 10) |&gt; \n  group_by(dimension) |&gt; \n  top_n(10, abs(value)) |&gt; \n  ungroup() |&gt; \n  mutate(word = reorder_within(word, value, dimension)) |&gt; \n  ggplot(aes(word, value, fill = dimension)) +\n  geom_col(alpha = 0.8, show.legend = FALSE) +\n  facet_wrap(~dimension, scales = \"free_y\", ncol = 3) +\n  scale_x_reordered() +\n  coord_flip() +\n  labs(\n    x = NULL, \n    y = \"Value\",\n    title = \"9 топиков Другой Розы\",\n    subtitle = \"Топ-9 слов\"\n  ) +\n  scale_fill_viridis_c()\nТопики с 10 по 19\nclean_rp_emb |&gt; \n  filter(dimension &lt; 20 & dimension &gt; 10) |&gt; \n  group_by(dimension) |&gt; \n  top_n(10, abs(value)) |&gt; \n  ungroup() |&gt; \n  mutate(word = reorder_within(word, value, dimension)) |&gt; \n  ggplot(aes(word, value, fill = dimension)) +\n  geom_col(alpha = 0.8, show.legend = FALSE) +\n  facet_wrap(~dimension, scales = \"free_y\", ncol = 3) +\n  scale_x_reordered() +\n  coord_flip() +\n  labs(\n    x = NULL, \n    y = \"Value\",\n    title = \"9 топиков Другой Розы\",\n    subtitle = \"Слова с 10 по 19\"\n  ) +\n  scale_fill_viridis_c()\nДля сравнения глубины исследования посмотрим на абсолютную частотность лемматизированного и токенизированного текста “Другой Розы”\n# Смотрим абсолютную частотность лемматизированного текста\nlibrary(ggplot2)\n\nall_roza_tidy |&gt; \n  count(word, sort = TRUE) |&gt; \n  slice_head(n = 50) |&gt; \n  ggplot(aes(reorder(word, n), n, fill = word)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  labs(x = NULL, y = NULL)\n# Абсолютная частотность токенизированного текста\nrp_token_tidy |&gt; \n  count(word, sort = TRUE) |&gt; \n  slice_head(n = 50) |&gt; \n  ggplot(aes(reorder(word, n), n, fill = word)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  labs(x = NULL, y = NULL)\n7. Блишайшие соседи\n# Загружаем заранее сохраненную функцию ближайшие соседи\n\nsource(\"docs/nearest_neighbors.R\")\n\n\nmimi_neighbors &lt;- clean_rp_emb |&gt; \n  nearest_neighbors(\"mimi_мими\") # я узнала кто такая МИМИ!!!!"
  },
  {
    "objectID": "research.html#quarto",
    "href": "research.html#quarto",
    "title": "Исследования",
    "section": "",
    "text": "library(tidyverse)\nlibrary(rvest)\nlibrary(tidytext)\nlibrary(tokenizers)\nlibrary(udpipe)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(wordcloud)\nlibrary(readr)\n\n\nall_roza_tidy &lt;- read.csv(\"docs/all_roza_tidy.csv\")\nrp_token_tidy &lt;- read.csv(\"docs/rp_token_tidy.csv\")\n\n\n# Смотрим абсолютную частотность лемматизированного текста\nlibrary(ggplot2)\n\nall_roza_tidy |&gt; \n  count(word, sort = TRUE) |&gt; \n  slice_head(n = 50) |&gt; \n  ggplot(aes(reorder(word, n), n, fill = word)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  labs(x = NULL, y = NULL)\n\n\n\n\n\n\n\n\n\n# Абсолютная частотность токенизированного текста\nrp_token_tidy |&gt; \n  count(word, sort = TRUE) |&gt; \n  slice_head(n = 50) |&gt; \n  ggplot(aes(reorder(word, n), n, fill = word)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  labs(x = NULL, y = NULL)"
  },
  {
    "objectID": "personalities.html#лео-йогишес",
    "href": "personalities.html#лео-йогишес",
    "title": "Персоналии",
    "section": "Лео Йогишес",
    "text": "Лео Йогишес\n\n\nDZIO_ДзиоДзио. [Позже добавлю фото и биографию].\n\n\n#Лео Йогишес\n\nyogishes_qty &lt;- all_roza_ann_tbl |&gt; \nfilter(lemma == \"йогишес\" | lemma == \"Йогишес\" \n         | lemma == \"иогишес\" | lemma == \"иогихес\"\n         | lemma == \"Лео\" | lemma == \"лео\") |&gt; \n  # select(doc_id, lemma, token, upos, feats) |&gt; \n  select(lemma, token)\n\n# он же, но уже под другим именем \ndzio_qty &lt;- all_roza_ann_tbl |&gt; \n  filter(str_detect(lemma, \"DZIO_\")) |&gt; \n  # select(doc_id, lemma, token, upos, feats)\n  select(lemma, token)\n\n# объединяем Лео\nleo_yogishes_qty &lt;- bind_rows(\n  yogishes_qty,\n  dzio_qty\n)\n\nunique(leo_yogishes_qty$token)\n\n [1] \"Лео\"              \"Йогишес\"          \"Йогишесу\"         \"DZIO_Циуциа\"     \n [5] \"DZIO_Дциодцио\"    \"DZIO_Циуция\"      \"DZIO_Дциодциу\"    \"DZIO_Циуцихну\"   \n [9] \"DZIO_Циуциухна\"   \"DZIO_Дциодциука\"  \"DZIO_Дциодциус\"   \"DZIO_Дциодциа\"   \n[13] \"DZIO_Дциодциухна\" \"DZIO_Дциодциниа\"  \"DZIO_Диудиу\"      \"DZIO_Диудиуку\"   \n\n# Сортируем имена Лео Йогишеса по частоте использования в повести.\n\nleo_yogishes_qty |&gt; \n  count(token) |&gt;  # Считаем token\n  arrange(desc(n))"
  },
  {
    "objectID": "personalities.html#костя-цеткин",
    "href": "personalities.html#костя-цеткин",
    "title": "Персоналии",
    "section": "Костя Цеткин",
    "text": "Костя Цеткин\n\n\nNIU_Ниуниу [Позже добавлю фото и биографию].\n\n\n# Ищем Костю Цеткина\nkostya_qty &lt;- all_roza_ann_tbl |&gt; \n  filter(str_detect(lemma, \"KOSTYA_\")) |&gt; \n  # select(doc_id, lemma, token, upos, feats) |&gt; \n  select(lemma, token)\n\n# он же\nniu_qty &lt;- all_roza_ann_tbl |&gt; \n  filter(str_detect(lemma, \"NIU_\")) |&gt; \n  # select(doc_id, lemma, token, upos, feats)|&gt;\n  select(lemma, token)\n\n# объединяем Костю\nkostya_tsetkin_qty &lt;- bind_rows(\n  kostya_qty,\n  niu_qty\n  )\n\nunique(kostya_tsetkin_qty$token)\n\n [1] \"KOSTYA_Косте\"      \"KOSTYA_Костю\"      \"KOSTYA_Костя\"     \n [4] \"KOSTYA_Костей\"     \"KOSTYA_Костик\"     \"KOSTYA_Кости\"     \n [7] \"KOSTYA_Костюша\"    \"KOSTYA_Костины\"    \"KOSTYA_Костин\"    \n[10] \"KOSTYA_Костиного\"  \"KOSTYA_Костиным\"   \"KOSTYA_Костина\"   \n[13] \"KOSTYA_костюм\"     \"KOSTYA_Костерское\" \"KOSTYA_костюме\"   \n[16] \"KOSTYA_костях\"     \"NIU_Ниуниу\"        \"NIU_Ниуниа\"       \n[19] \"NIU_Ниуника\"       \"NIU_Ниуниуку\"      \"NIU_Ниуниус\"      \n[22] \"NIU_Нииуниу\"       \"NIU_Ниунисия\"      \"NIU_Ниунийчика\"   \n[25] \"NIU_Ниуниуш\"      \n\n# Есть ошибки присвоения маркеров, но их доля невелика\nkostya_tsetkin_qty |&gt; \n  count(token) |&gt;  # Считаем token\n  arrange(desc(n))"
  },
  {
    "objectID": "personalities.html#мими",
    "href": "personalities.html#мими",
    "title": "Персоналии",
    "section": "Мими",
    "text": "Мими\n\n\nЗагадочная Мими. При лемматизации эта Мими упорно превращалась в слово “мой”. Заглянула в тексты, где она упоминается - выходило, что это какое-то животное? Обратим на нее внимание! Продолжение ее истории дальше.\n\n\n# Ищем собачку/кошечку Мими, лемматизированную как \"мой\"\nmimi_qty &lt;- all_roza_ann_tbl |&gt; \n  filter(str_detect(lemma, \"MIMI_\")) |&gt; \n  select(doc_id, lemma, token, upos, feats)\n\nmimi_qty |&gt; \n  count(token) |&gt;  # Считаем token\n  arrange(desc(n))"
  },
  {
    "objectID": "research.html#векторное-представление-слов",
    "href": "research.html#векторное-представление-слов",
    "title": "Исследования",
    "section": "Векторное представление слов",
    "text": "Векторное представление слов\n Строим эмбеддинги, взяв за основу матрицу термин-термин.\n\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(tidytext)\nlibrary(tokenizers)\nlibrary(udpipe)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(wordcloud)\nlibrary(readr)\n\n\n1. Скользящее окно\n\n# Загружаем заранее сохраненные данные\nall_roza_tidy &lt;- read.csv(\"docs/all_roza_tidy.csv\")\nrp_token_tidy &lt;- read.csv(\"docs/rp_token_tidy.csv\")\n\n# \"Гнездуем\")) токены по главам повести для дальнейшего деления на окна\n#nested_proza &lt;- all_roza_tokens |&gt; \n#  dplyr::select(-link) |&gt; \n#  nest(token = c(word))\n\n# NEW \"Гнездуем\" почищенные токены\nclean_nested_proza &lt;- rp_token_tidy |&gt; \n  dplyr::select(-link) |&gt; \n  nest(token = c(word))\n\n\n2. Создаем функцию для деления со сдвигом\n\nslide_windows &lt;- function(tbl, window_size) {\n  skipgrams &lt;- slider::slide(\n    tbl, \n    ~.x, \n    .after = window_size - 1, \n    .step = 1, \n    .complete = TRUE\n  )\n  \n  safe_mutate &lt;- safely(mutate)\n  \n  out &lt;- map2(.x = skipgrams,\n              .y = 1:length(skipgrams), # Генерация последовательности индексов\n              ~ safe_mutate(.x, window_id = .y))\n  \n  out |&gt; \n    transpose() |&gt; \n    pluck(\"result\") |&gt; \n    compact() |&gt; \n    bind_rows()\n}\n\n\n3. Делим на окна\n\n# Ширина окна 10L - .after = window_size - 1 дает 11.\n# unnest - распаковываем токены\n# unite - достаем слова из столбца со словами и у нас отдельно window_id и название документа\n#proza_windows &lt;- nested_proza |&gt; \n#  mutate(token = map(token, slide_windows, 10L)) |&gt;  \n#  unnest(token) |&gt;  \n#  unite(window_id, title, window_id)\n\n\n# NEW То же самое делаем с почищенным файлом\nclean_proza_windows &lt;- clean_nested_proza |&gt; \n  mutate(token = map(token, slide_windows, 10L)) |&gt;  \n  unnest(token) |&gt;  \n  unite(window_id, title, window_id)\n\n\n4. Считаем PMI и PPMI\n\nlibrary(widyr)\n\nclean_rp_pmi &lt;- clean_proza_windows |&gt;\n  pairwise_pmi(word, window_id)\n\n# всего значений/токенов/координат\nclean_rp_pmi$item1 |&gt; unique() |&gt; length() # 20710\n\n[1] 20710\n\n# Positive PPMI\nclean_rp_ppmi &lt;- clean_rp_pmi |&gt; \n  mutate(ppmi = case_when(pmi &lt; 0 ~ 0,\n                          .default = pmi))\n\nclean_rp_ppmi |&gt; \n  arrange(pmi)\n\n# A tibble: 1,010,118 × 4\n   item1     item2       pmi  ppmi\n   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 особенно  roza_роза -3.75     0\n 2 roza_роза особенно  -3.75     0\n 3 могу      получила  -3.40     0\n 4 получила  могу      -3.40     0\n 5 нам       roza_роза -3.32     0\n 6 roza_роза нам       -3.32     0\n 7 хочу      roza_розы -3.31     0\n 8 roza_розы хочу      -3.31     0\n 9 другие    roza_роза -3.25     0\n10 roza_роза другие    -3.25     0\n# ℹ 1,010,108 more rows\n\n\n\n5. SVD на матрице с PPMI\n\nclean_rp_emb &lt;- clean_rp_ppmi |&gt; \n  widely_svd(item1, item2, ppmi,\n             weight_d = FALSE, nv = 100) |&gt; \n  rename(word = item1) # иначе nearest_neighbors() будет жаловаться\n\n\n6. Визуализация топиков\nТопики с 1 по 9\n\nclean_rp_emb |&gt; \n  filter(dimension &lt; 10) |&gt; \n  group_by(dimension) |&gt; \n  top_n(10, abs(value)) |&gt; \n  ungroup() |&gt; \n  mutate(word = reorder_within(word, value, dimension)) |&gt; \n  ggplot(aes(word, value, fill = dimension)) +\n  geom_col(alpha = 0.8, show.legend = FALSE) +\n  facet_wrap(~dimension, scales = \"free_y\", ncol = 3) +\n  scale_x_reordered() +\n  coord_flip() +\n  labs(\n    x = NULL, \n    y = \"Value\",\n    title = \"9 топиков Другой Розы\",\n    subtitle = \"Топ-9 слов\"\n  ) +\n  scale_fill_viridis_c()\n\n\n\n\n\n\n\n\nТопики с 10 по 19\n\nclean_rp_emb |&gt; \n  filter(dimension &lt; 20 & dimension &gt; 10) |&gt; \n  group_by(dimension) |&gt; \n  top_n(10, abs(value)) |&gt; \n  ungroup() |&gt; \n  mutate(word = reorder_within(word, value, dimension)) |&gt; \n  ggplot(aes(word, value, fill = dimension)) +\n  geom_col(alpha = 0.8, show.legend = FALSE) +\n  facet_wrap(~dimension, scales = \"free_y\", ncol = 3) +\n  scale_x_reordered() +\n  coord_flip() +\n  labs(\n    x = NULL, \n    y = \"Value\",\n    title = \"9 топиков Другой Розы\",\n    subtitle = \"Слова с 10 по 19\"\n  ) +\n  scale_fill_viridis_c()\n\n\n\n\n\n\n\n\nДля сравнения глубины исследования посмотрим на абсолютную частотность лемматизированного и токенизированного текста “Другой Розы”\n\n# Смотрим абсолютную частотность лемматизированного текста\nlibrary(ggplot2)\n\nall_roza_tidy |&gt; \n  count(word, sort = TRUE) |&gt; \n  slice_head(n = 50) |&gt; \n  ggplot(aes(reorder(word, n), n, fill = word)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  labs(x = NULL, y = NULL)\n\n\n\n\n\n\n\n\n\n# Абсолютная частотность токенизированного текста\nrp_token_tidy |&gt; \n  count(word, sort = TRUE) |&gt; \n  slice_head(n = 50) |&gt; \n  ggplot(aes(reorder(word, n), n, fill = word)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  labs(x = NULL, y = NULL)"
  },
  {
    "objectID": "research.html#визуализируем-мими-точнее-ее-соседей",
    "href": "research.html#визуализируем-мими-точнее-ее-соседей",
    "title": "Исследования",
    "section": "Визуализируем Мими (точнее, ее соседей!)",
    "text": "Визуализируем Мими (точнее, ее соседей!)\n\n#облако слов - визуализируем МИМИ!!\nlibrary(wordcloud2)\npal &lt;- c( \"#9B251E\", \n          \"#FFB3BA\", \"#FFDFBA\", \"#BAFFBF\",\n          \"#BAE1FF\", \"#FFE788\", \"#9B251E\", \n          \"#BAFFBF\", \"#BAE1FF\", \"#FFE788\", \n          \"#FFB3BA\", \"#FFDFBA\",\n          \"#BAE1FF\", \"#9B251E\", \"#FFE788\", \"#FFDFBA\")\n\nwordcloud2(data = mimi_neighbors,\n           size = 1,                           \n           color = pal,                        \n           backgroundColor = \"white\", \n           shape = 'circle',                   \n           minSize = 1)\n\n\n\n\n\n\n7. 2D визуализация пространства слов\n\n# 2D визуализация пространства слов\nlibrary(umap)\n\n# Преобразование данных в матрицу\nrp_emb_mx &lt;- clean_rp_emb |&gt; \n  cast_sparse(word, dimension, value) |&gt; \n  as.matrix()\n\n\n# UMAP2, снизили количество n_neighbors c 15 до 10\nset.seed(02062024)\nviz2 &lt;- umap(rp_emb_mx, n_neighbors = 10, n_threads = 2)$layout\n\n\n# Проверяем размеры результата\ndim(viz2) # 20700 - 2\n\n[1] 20710     2\n\n# Построение графика viz2 - работает, увеличили ширину и высоту position_jitter\ntibble(word = rownames(rp_emb_mx), \n       V1 = viz2[, 1], \n       V2 = viz2[, 2]) |&gt; \n  ggplot(aes(x = V1, y = V2, label = word)) + \n  geom_text(size = 2, alpha = 0.4, position = position_jitter(width = 1, height = 1)) +\n  annotate(geom = \"rect\", ymin = 2.5, ymax = 7, xmin = 1.5, xmax = 6.5, alpha = 0.2, color = \"tomato\") +\n  theme_light()\n\n\n\n\n\n\n\n# приближение\ntibble(word = rownames(rp_emb_mx), \n       V1 = viz2[, 1], \n       V2 = viz2[, 2]) |&gt; \n  filter(V1 &gt; 1.5 & V1 &lt; 6.5) |&gt; \n  filter(V2 &gt; 2.5 & V2 &lt; 7) |&gt; \n  ggplot(aes(x = V1, y = V2, label = word)) + \n  geom_text(size = 2, alpha = 0.4, position = position_jitter(width = 0.5, height = 0.5)) +\n  theme_light()\n\n\n\n\n\n\n\n\n\nПромежуточный итог исследования\nНа осмысление и просто даже фиксацию всех находок, наблюдений и гипотез времени не хватило. На последней закладке сайта планировалось поместить найденные интересные тексты из корпуса, надеюсь это в ближайшем будущем доделать.\nЧтобы закончить на высокой ноте привожу “ближайших соседей” облаков.\n\nsky_neighbors &lt;- clean_rp_emb |&gt; \n  nearest_neighbors(\"небе\")\n\nsky_neighbors"
  }
]