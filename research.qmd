---
title: "Исследования"
subtitle: "Переходим к исследованию полученных данных."
format: html
warning: FALSE
editor: visual
---

<br/>
## Векторное представление слов

<br/> 
Строим эмбеддинги, взяв за основу матрицу термин-термин.

```{r}
library(tidyverse)
library(rvest)
library(tidytext)
library(tokenizers)
library(udpipe)
library(dplyr)
library(ggplot2)
library(wordcloud)
library(readr)

```

<br/>

**1. Скользящее окно**

```{r}
# Загружаем заранее сохраненные данные
all_roza_tidy <- read.csv("docs/all_roza_tidy.csv")
rp_token_tidy <- read.csv("docs/rp_token_tidy.csv")

# "Гнездуем")) токены по главам повести для дальнейшего деления на окна
#nested_proza <- all_roza_tokens |> 
#  dplyr::select(-link) |> 
#  nest(token = c(word))

# NEW "Гнездуем" почищенные токены
clean_nested_proza <- rp_token_tidy |> 
  dplyr::select(-link) |> 
  nest(token = c(word))
```

<br/>

**2. Создаем функцию для деления со сдвигом**

```{r}
slide_windows <- function(tbl, window_size) {
  skipgrams <- slider::slide(
    tbl, 
    ~.x, 
    .after = window_size - 1, 
    .step = 1, 
    .complete = TRUE
  )
  
  safe_mutate <- safely(mutate)
  
  out <- map2(.x = skipgrams,
              .y = 1:length(skipgrams), # Генерация последовательности индексов
              ~ safe_mutate(.x, window_id = .y))
  
  out |> 
    transpose() |> 
    pluck("result") |> 
    compact() |> 
    bind_rows()
}

```

<br/>

**3. Делим на окна**

```{r}
# Ширина окна 10L - .after = window_size - 1 дает 11.
# unnest - распаковываем токены
# unite - достаем слова из столбца со словами и у нас отдельно window_id и название документа
#proza_windows <- nested_proza |> 
#  mutate(token = map(token, slide_windows, 10L)) |>  
#  unnest(token) |>  
#  unite(window_id, title, window_id)


# NEW То же самое делаем с почищенным файлом
clean_proza_windows <- clean_nested_proza |> 
  mutate(token = map(token, slide_windows, 10L)) |>  
  unnest(token) |>  
  unite(window_id, title, window_id)
```

<br/>

**4. Считаем PMI и PPMI**

```{r}
library(widyr)

clean_rp_pmi <- clean_proza_windows |>
  pairwise_pmi(word, window_id)

# всего значений/токенов/координат
clean_rp_pmi$item1 |> unique() |> length() # 20710


# Positive PPMI
clean_rp_ppmi <- clean_rp_pmi |> 
  mutate(ppmi = case_when(pmi < 0 ~ 0,
                          .default = pmi))

clean_rp_ppmi |> 
  arrange(pmi)
```
<br/>

**5. SVD на матрице с PPMI**

```{r}
clean_rp_emb <- clean_rp_ppmi |> 
  widely_svd(item1, item2, ppmi,
             weight_d = FALSE, nv = 100) |> 
  rename(word = item1) # иначе nearest_neighbors() будет жаловаться

```
<br/>

**6. Визуализация топиков**

Топики с 1 по 9

```{r}
clean_rp_emb |> 
  filter(dimension < 10) |> 
  group_by(dimension) |> 
  top_n(10, abs(value)) |> 
  ungroup() |> 
  mutate(word = reorder_within(word, value, dimension)) |> 
  ggplot(aes(word, value, fill = dimension)) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~dimension, scales = "free_y", ncol = 3) +
  scale_x_reordered() +
  coord_flip() +
  labs(
    x = NULL, 
    y = "Value",
    title = "9 топиков Другой Розы",
    subtitle = "Топ-9 слов"
  ) +
  scale_fill_viridis_c()

```

Топики с 10 по 19

```{r}
clean_rp_emb |> 
  filter(dimension < 20 & dimension > 10) |> 
  group_by(dimension) |> 
  top_n(10, abs(value)) |> 
  ungroup() |> 
  mutate(word = reorder_within(word, value, dimension)) |> 
  ggplot(aes(word, value, fill = dimension)) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~dimension, scales = "free_y", ncol = 3) +
  scale_x_reordered() +
  coord_flip() +
  labs(
    x = NULL, 
    y = "Value",
    title = "9 топиков Другой Розы",
    subtitle = "Слова с 10 по 19"
  ) +
  scale_fill_viridis_c()

```


Для сравнения глубины исследования посмотрим на абсолютную частотность лемматизированного и токенизированного текста "Другой Розы"

```{r}
# Смотрим абсолютную частотность лемматизированного текста
library(ggplot2)

all_roza_tidy |> 
  count(word, sort = TRUE) |> 
  slice_head(n = 50) |> 
  ggplot(aes(reorder(word, n), n, fill = word)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(x = NULL, y = NULL)

```

```{r}
# Абсолютная частотность токенизированного текста
rp_token_tidy |> 
  count(word, sort = TRUE) |> 
  slice_head(n = 50) |> 
  ggplot(aes(reorder(word, n), n, fill = word)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(x = NULL, y = NULL)

```
<br/>

**7. Блишайшие соседи**

```{r}
# Загружаем заранее сохраненную функцию ближайшие соседи

source("docs/nearest_neighbors.R")


mimi_neighbors <- clean_rp_emb |> 
  nearest_neighbors("mimi_мими") # я узнала кто такая МИМИ!!!!

```

## Визуализируем Мими (точнее, ее соседей!)

```{r}
#облако слов - визуализируем МИМИ!!
library(wordcloud2)
pal <- c( "#9B251E", 
          "#FFB3BA", "#FFDFBA", "#BAFFBF",
          "#BAE1FF", "#FFE788", "#9B251E", 
          "#BAFFBF", "#BAE1FF", "#FFE788", 
          "#FFB3BA", "#FFDFBA",
          "#BAE1FF", "#9B251E", "#FFE788", "#FFDFBA")

wordcloud2(data = mimi_neighbors,
           size = 1,                           
           color = pal,                        
           backgroundColor = "white", 
           shape = 'circle',                   
           minSize = 1)
```